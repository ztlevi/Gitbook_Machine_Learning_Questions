
<!DOCTYPE HTML>
<html lang>
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>How to prevent overfitting &#xB7; GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content>
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-custom-js-css/9eb07385451571ed1c43d6bfc772c8d5-docsearch.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-docsearch/doc-search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-intopic-toc/style.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="linear_algebra.html">
    
    
    <link rel="prev" href="optimization.html">
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary"><div id="book-search-input">
    <input type="text" id="book-doc-search-input" placeholder="Type to search">
</div>
        
            
            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="./">
            
                <a href="./">
            
                    
                    Basics
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="optimization.html">
            
                <a href="optimization.html">
            
                    
                    Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.2" data-path="how_to_prevent_overfitting.html">
            
                <a href="how_to_prevent_overfitting.html">
            
                    
                    How to prevent overfitting
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="linear_algebra.html">
            
                <a href="linear_algebra.html">
            
                    
                    Linear Algebra
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="clustering.html">
            
                <a href="clustering.html">
            
                    
                    Clustering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="calculate_parameters_in_cnn.html">
            
                <a href="calculate_parameters_in_cnn.html">
            
                    
                    Calculate Parameters in CNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="weight_norm_and_layer_norm.html">
            
                <a href="weight_norm_and_layer_norm.html">
            
                    
                    Weight norm and layer norm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="Confidence_Interval.html">
            
                <a href="Confidence_Interval.html">
            
                    
                    Confidence Interval
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="Quantization.html">
            
                <a href="Quantization.html">
            
                    
                    Quantization
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../classical_ml/">
            
                <a href="../classical_ml/">
            
                    
                    Classical Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../neural_networks/">
            
                <a href="../neural_networks/">
            
                    
                    Neural Networks
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../neural_networks/different_types_of_convolution.html">
            
                <a href="../neural_networks/different_types_of_convolution.html">
            
                    
                    Different Types of Convolution
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../neural_networks/loss/">
            
                <a href="../neural_networks/loss/">
            
                    
                    Loss
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.2.1" data-path="../neural_networks/loss/Hinge_Loss.html">
            
                <a href="../neural_networks/loss/Hinge_Loss.html">
            
                    
                    Hinge Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.2" data-path="../neural_networks/loss/cross_entropy_loss.html">
            
                <a href="../neural_networks/loss/cross_entropy_loss.html">
            
                    
                    Cross-Entropy Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.3" data-path="../neural_networks/loss/binary_cross_entropy_loss.html">
            
                <a href="../neural_networks/loss/binary_cross_entropy_loss.html">
            
                    
                    Binary Cross-Entropy Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.4" data-path="../neural_networks/loss/categorical_cross_entropy_loss.html">
            
                <a href="../neural_networks/loss/categorical_cross_entropy_loss.html">
            
                    
                    Categorical Cross-Entropy Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.5" data-path="../neural_networks/loss/focal_loss.html">
            
                <a href="../neural_networks/loss/focal_loss.html">
            
                    
                    Optional: Focal Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.6" data-path="../neural_networks/loss/coral_loss.html">
            
                <a href="../neural_networks/loss/coral_loss.html">
            
                    
                    Optional: CORAL Loss
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../neural_networks/resnet.html">
            
                <a href="../neural_networks/resnet.html">
            
                    
                    Resnet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../neural_networks/mobilenet.html">
            
                <a href="../neural_networks/mobilenet.html">
            
                    
                    Mobilenet
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../cv/">
            
                <a href="../cv/">
            
                    
                    Computer Vision
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../cv/two-stage-detector/">
            
                <a href="../cv/two-stage-detector/">
            
                    
                    Two Stage Object Detection
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1.1" data-path="../cv/two-stage-detector/metrics.html">
            
                <a href="../cv/two-stage-detector/metrics.html">
            
                    
                    Metrics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.2" data-path="../cv/two-stage-detector/roi.html">
            
                <a href="../cv/two-stage-detector/roi.html">
            
                    
                    ROI
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.3" data-path="../cv/two-stage-detector/r-cnn.html">
            
                <a href="../cv/two-stage-detector/r-cnn.html">
            
                    
                    R-CNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.4" data-path="../cv/two-stage-detector/fast-rcnn.html">
            
                <a href="../cv/two-stage-detector/fast-rcnn.html">
            
                    
                    Fast RCNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.5" data-path="../cv/two-stage-detector/faster-rcnn.html">
            
                <a href="../cv/two-stage-detector/faster-rcnn.html">
            
                    
                    Faster RCNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.6" data-path="../cv/two-stage-detector/mask-rcnn.html">
            
                <a href="../cv/two-stage-detector/mask-rcnn.html">
            
                    
                    Mask RCNN
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../cv/one-stage-detector/">
            
                <a href="../cv/one-stage-detector/">
            
                    
                    One Stage Object Detection
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.2.1" data-path="../cv/one-stage-detector/yolo.html">
            
                <a href="../cv/one-stage-detector/yolo.html">
            
                    
                    YOLO
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2.2" data-path="../cv/one-stage-detector/ssd.html">
            
                <a href="../cv/one-stage-detector/ssd.html">
            
                    
                    Single Shot MultiBox Detector(SSD)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2.3" data-path="../cv/one-stage-detector/fpn.html">
            
                <a href="../cv/one-stage-detector/fpn.html">
            
                    
                    FPN
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../cv/segmentation/">
            
                <a href="../cv/segmentation/">
            
                    
                    Segmentation
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.3.1" data-path="../cv/segmentation/panoptic.html">
            
                <a href="../cv/segmentation/panoptic.html">
            
                    
                    Panoptic Segmentation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3.2" data-path="../cv/segmentation/PSPNet.html">
            
                <a href="../cv/segmentation/PSPNet.html">
            
                    
                    PSPNet
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="../cv/facenet.html">
            
                <a href="../cv/facenet.html">
            
                    
                    FaceNet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="../cv/gan.html">
            
                <a href="../cv/gan.html">
            
                    
                    GAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" data-path="../cv/Object_detection_imbalance.html">
            
                <a href="../cv/Object_detection_imbalance.html">
            
                    
                    Imbalance problem in object detection
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../nlp/">
            
                <a href="../nlp/">
            
                    
                    NLP
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../nlp/embedding.html">
            
                <a href="../nlp/embedding.html">
            
                    
                    Embedding
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../nlp/rnn.html">
            
                <a href="../nlp/rnn.html">
            
                    
                    RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="../nlp/lstm.html">
            
                <a href="../nlp/lstm.html">
            
                    
                    LSTM
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../parallel-computeing/">
            
                <a href="../parallel-computeing/">
            
                    
                    Parallel Computeing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../parallel-computeing/communication.html">
            
                <a href="../parallel-computeing/communication.html">
            
                    
                    Communication
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../parallel-computeing/sync_mapreduce.html">
            
                <a href="../parallel-computeing/sync_mapreduce.html">
            
                    
                    MapReduce
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../parallel-computeing/parameter_server.html">
            
                <a href="../parallel-computeing/parameter_server.html">
            
                    
                    Parameter Server
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../parallel-computeing/decentralized_and_ring_allreduce.html">
            
                <a href="../parallel-computeing/decentralized_and_ring_allreduce.html">
            
                    
                    Decentralized And Ring All Reduce
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../parallel-computeing/federated_learning.html">
            
                <a href="../parallel-computeing/federated_learning.html">
            
                    
                    Federated Learning
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../anomaly-detection/">
            
                <a href="../anomaly-detection/">
            
                    
                    Anomaly Detection
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="../anomaly-detection/dbscan.html">
            
                <a href="../anomaly-detection/dbscan.html">
            
                    
                    DBSCAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="../anomaly-detection/autoencoder_anomaly.html">
            
                <a href="../anomaly-detection/autoencoder_anomaly.html">
            
                    
                    Autoencoder
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../visualization/">
            
                <a href="../visualization/">
            
                    
                    Visualization
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="../visualization/saliency_maps.html">
            
                <a href="../visualization/saliency_maps.html">
            
                    
                    Saliency Maps
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="../visualization/fooling_images.html">
            
                <a href="../visualization/fooling_images.html">
            
                    
                    Fooling images
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="../visualization/class_visualization.html">
            
                <a href="../visualization/class_visualization.html">
            
                    
                    Class Visualization
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="..">How to prevent overfitting</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h2 id="bias-variance-trade-off">Bias Variance Trade-Off</h2>
<h3 id="bias-and-variance">Bias and Variance</h3>
<p><img src="../.gitbook/assets/1*xwtSpR_zg7j7zusa4IDHNQ.png" alt="img"></p>
<ul>
<li>Bias is the difference between the average prediction of our model and the correct value which we are trying to predict.</li>
<li>Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn&#x2019;t seen before.</li>
</ul>
<h3 id="trade-off">Trade-Off</h3>
<p><img src="../.gitbook/assets/degree_of_polynomial_d.png" alt="img"></p>
<ul>
<li>if d is too small --&gt; this probably corresponds to a high bias problem</li>
<li>if d is too large --&gt; this probably corresponds to a high variance problem</li>
</ul>
<p><img src="../.gitbook/assets/bias-variance-total-error.jpg" alt="tradeoff"></p>
<p>Predictive models have a tradeoff between <strong>bias</strong> (how well the model fits the data) and <strong>variance</strong> (how much the model changes based on changes in the inputs).</p>
<p><em>Simpler models</em> are stable (low variance) but they don&apos;t get close to the truth (high bias).</p>
<p>More <em>complex models</em> are more prone to being overfit (high variance) but they are expressive enough to get close to the truth (low bias).</p>
<p>The best model for a given problem usually lies somewhere in the middle.</p>
<ul>
<li><strong>underfitting</strong> happens when a model unable to capture the underlying pattern of the data.</li>
<li><strong>overfitting</strong> happens when our model captures the noise along with the underlying pattern in data.</li>
</ul>
<h2 id="how-to-prevent-overfitting">How to Prevent Overfitting</h2>
<p>Detecting overfitting is useful, but it doesn&apos;t solve the problem. Fortunately, you have several options to try.</p>
<p>Here are a few of the most popular solutions for overfitting:</p>
<h3 id="cross-validation">Cross-validation</h3>
<p>Cross-validation is a powerful preventative measure against overfitting.</p>
<p>The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.</p>
<p>In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the &quot;holdout fold&quot;).</p>
<p><img src="https://elitedatascience.com/wp-content/uploads/2017/06/Cross-Validation-Diagram.jpg" alt="K-Fold Cross-Validation"></p>
<p>K-Fold Cross-Validation</p>
<p>Cross-validation allows you to tune hyperparameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.</p>
<p>We have another article with a <a href="https://elitedatascience.com/machine-learning-iteration#micro" target="_blank">more detailed breakdown of cross-validation</a>.</p>
<h3 id="train-with-more-data">Train with more data</h3>
<p>It won&apos;t work every time, but training with more data can help algorithms detect the signal better. In the earlier example of modeling height vs. age in children, it&apos;s clear how sampling more schools will help your model.</p>
<p>Of course, that&apos;s not always the case. If we just add more noisy data, this technique won&apos;t help. That&apos;s why you should always ensure your data is clean and relevant.</p>
<h3 id="remove-features">Remove features</h3>
<p>Some algorithms have built-in feature selection.</p>
<p>For those that don&apos;t, you can manually improve their generalizability by removing irrelevant input features.</p>
<p>An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist&apos;s spin on software engineer&apos;s <a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging" target="_blank">rubber duck debugging</a> technique, where they debug their code by explaining it, line-by-line, to a rubber duck.</p>
<p>If anything doesn&apos;t make sense, or if it&apos;s hard to justify certain features, this is a good way to identify them.</p>
<p>In addition, there are several <a href="https://elitedatascience.com/dimensionality-reduction-algorithms#feature-selection" target="_blank">feature selection heuristics</a> you can use for a good starting point.</p>
<h3 id="early-stopping">Early stopping</h3>
<p>When you&apos;re <a href="https://elitedatascience.com/machine-learning-iteration#model" target="_blank">training a learning algorithm iteratively</a>, you can measure how well each iteration of the model performs.</p>
<p>Up until a certain number of iterations, new iterations improve the model. After that point, however, the model&apos;s ability to generalize can weaken as it begins to overfit the training data.</p>
<p>Early stopping refers stopping the training process before the learner passes that point.</p>
<p><img src="https://elitedatascience.com/wp-content/uploads/2017/09/early-stopping-graphic.jpg" alt="Early stopping graphic"></p>
<p>Today, this technique is mostly used in deep learning while other techniques (e.g. regularization) are preferred for classical machine learning.</p>
<h3 id="regularization">Regularization</h3>
<p>Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.</p>
<p>Shrinkage can be thought of as &quot;a penalty of complexity.&quot; Why? If we set some parameters of the model to exactly zero, then the model is effectively shrunk to have lower-dimensionality and less complex. Analogously, if we use a shrinkage mechanism to <strong>zero out some of the parameters</strong> or <strong>smooth the parameters</strong> (the difference of parameters will not be very large), then we are decreasing complexity by reducing dimensions or making it more continuous.</p>
<h4 id="l1-regularization-or-lasso-or-l1-norm">L1 Regularization or Lasso or L1 norm</h4>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo>)</mo><mo>=</mo><msubsup><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>&#x2212;</mo><msub><mi>h</mi><mrow><mi>&#x3B8;</mi></mrow></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup><mo>+</mo><mi>&#x3BB;</mi><msubsup><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi mathvariant="normal">&#x2223;</mi><msub><mi>&#x3B8;</mi><mi>i</mi></msub><mi mathvariant="normal">&#x2223;</mi></mrow><annotation encoding="application/x-tex">
L(x,y) = \sum_{i=1}^n(y_i - h_{\theta}(x_i))^2 + \lambda \sum_{i=1}^n |\theta_i|
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.6513970000000002em;"></span><span class="strut bottom" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathit mtight">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord mathit mtight">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mbin">&#x2212;</span><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">&#x3B8;</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord mathit mtight">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathrm mtight">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mbin">+</span><span class="mord mathit">&#x3BB;</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathit mtight">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span><span class="mord mathrm">&#x2223;</span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">&#x3B8;</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord mathit mtight">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mord mathrm">&#x2223;</span></span></span></span></span></p>
<p>In L1 regularization we penalize the absolute value of the weights.</p>
<h4 id="l2-regularization-or-ridge-regularization">L2 Regularization or Ridge Regularization</h4>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo>)</mo><mo>=</mo><msubsup><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>&#x2212;</mo><msub><mi>h</mi><mrow><mi>&#x3B8;</mi></mrow></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup><mo>+</mo><mi>&#x3BB;</mi><msubsup><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>&#x3B8;</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">
L(x,y) = \sum_{i=1}^n(y_i - h_{\theta}(x_i))^2 + \lambda \sum_{i=1}^n \theta_i^2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.6513970000000002em;"></span><span class="strut bottom" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathit mtight">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord mathit mtight">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mbin">&#x2212;</span><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">&#x3B8;</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord mathit mtight">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathrm mtight">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mbin">+</span><span class="mord mathit">&#x3BB;</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span><span class="mop op-symbol large-op">&#x2211;</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathit mtight">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">&#x3B8;</span><span class="msupsub"><span class="vlist"><span style="top:0.247em;margin-left:-0.02778em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord mathit mtight">i</span></span></span><span style="top:-0.4129999999999999em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle uncramped mtight"><span class="mord mathrm mtight">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span></span></span></span></span></p>
<p>In L2 regularization, regularization term is the sum of square of all feature weights as shown above in the equation.</p>
<h4 id="comparison-between-l1-and-l2-regulariztion">Comparison Between L1 And L2 Regulariztion</h4>
<ul>
<li><strong>Computational Efficiency</strong>: (L2 &gt; L1) L2 have analytical solution while L1 is computational inefficient on non-sparse cases</li>
<li><strong>Sparsity</strong>: (L1 &gt; L2) refers to that only very few entries in a matrix (or vector) is non-zero. L1-norm has the property of producing many coefficients with zero values or very small values with few large coefficients.</li>
<li><p><strong>Built-in feature selection for L1</strong>: L1-norm tends to produces sparse coefficients, so that L1 can penalize the coefficients toward 0.</p>
<p><img src="../.gitbook/assets/cmWO0.png" alt="enter image description here"></p>
<p><img src="../.gitbook/assets/Mkclz.png" alt="enter image description here"></p>
</li>
</ul>
<p>From the figure, we can find out that for L1, the gradient is either 1 or -1, except for when <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w_{1}=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.79444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathrm mtight">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span>, however, for the same <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>&#x3BB;</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">&#x3BB;</span></span></span></span>, it is possible that the weight for L2 norm will never reach zero for the gradient of the weight is also very small, which may result in a smaller penalty for the weight.</p>
<h4 id="choosing-&#x3BB;">Choosing &#x3BB;</h4>
<p><img src="../.gitbook/assets/regularization_lambda.jpg" alt="img"></p>
<ul>
<li>Plot &#x3BB; vs.<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ train }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.09618em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight" style="margin-right:0.02778em;">r</span><span class="mord mathit mtight">a</span><span class="mord mathit mtight">i</span><span class="mord mathit mtight">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span></span></span></span><ul>
<li>When &#x3BB; is small you get a small value (regularization basically goes to 0)</li>
<li>When &#x3BB; is large you get a large vale corresponding to high bias</li>
</ul>
</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>c</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ cv }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.09618em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">c</span><span class="mord mathit mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span></span></span></span><ul>
<li>When &#x3BB; is small we see high variance<ul>
<li>Too small a value means we over fit the data</li>
</ul>
</li>
<li>When &#x3BB; is large we end up underfitting, so this is bias<ul>
<li>So cross validation error is high</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Such a plot can help show you you&apos;re picking a good value for &#x3BB;</li>
</ul>
<h4 id="dropout-regularization-technique">DropOut (Regularization technique)</h4>
<p>To apply DropOut, we randomly select a subset of the units and clamp their output to zero, regardless of the input; this effectively removes those units from the model. A different subset of units is randomly selected every time we present a training example.</p>
<p>Below are two possible network configurations. On the first presentation (left), the 1st and 3rd units are disabled, but the 2nd and 3rd units have been randomly selected on a subsequent presentation. At test time, we use the complete network but rescale the weights to compensate for the fact that all of them can now become active (e.g., if you drop half of the nodes, the weights should also be halved).</p>
<p><a href="https://i.stack.imgur.com/CewjH.png" target="_blank"><img src="https://i.stack.imgur.com/CewjH.png" alt="DropOut examples"></a></p>
<p>Cons:</p>
<ul>
<li>Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.</li>
</ul>
<h4 id="dropconnect">DropConnect</h4>
<p>DropConnect works similarly, except that we disable individual weights (i.e., set them to zero), instead of nodes, so a node can remain partially active. Schematically, it looks like this:</p>
<p><a href="https://i.stack.imgur.com/D1QC7.png" target="_blank"><img src="https://i.stack.imgur.com/D1QC7.png" alt="DropConnect"></a></p>
<h4 id="comparison">Comparison</h4>
<p>These methods both work because they effectively let you train several models at the same time, then average across them for testing. For example, the yellow layer has four nodes, and thus 16 possible DropOut states (all enabled, #1 disabled, #1 and #2 disabled, etc).</p>
<p>DropConnect is a generalization of DropOut because it produces even more possible models, since there are almost always more connections than units. However, you can get similar outcomes on an individual trial. For example, the DropConnect network on the right has effectively dropped Unit #2 since all of the incoming connections have been removed.</p>
<h3 id="batch-norm"><a href="https://github.com/ztlevi/Machine_Learning_Questions/tree/26cb30cb7a3ec95f737534585c8ae80567d03d7b/docs/General.html#batch-normalization" target="_blank">Batch Norm</a></h3>
<h3 id="ensembling">Ensembling</h3>
<p>Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:</p>
<p><em>Bagging</em> attempts to reduce the chance overfitting complex models.</p>
<ul>
<li>It trains a large number of &quot;strong&quot; learners in parallel.</li>
<li>A strong learner is a model that&apos;s relatively unconstrained.</li>
<li>Bagging then combines all the strong learners together in order to &quot;smooth out&quot; their predictions.</li>
</ul>
<p><em>Boosting</em> attempts to improve the predictive flexibility of simple models.</p>
<ul>
<li>It trains a large number of &quot;weak&quot; learners in sequence.</li>
<li>A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).</li>
<li>Each one in the sequence focuses on learning from the mistakes of the one before it.</li>
<li>Boosting then combines all the weak learners into a single strong learner.</li>
</ul>
<p>While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.</p>
<p>Bagging uses complex base models and tries to &quot;smooth out&quot; their predictions, while boosting uses simple base models and tries to &quot;boost&quot; their aggregate complexity.</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="optimization.html" class="navigation navigation-prev " aria-label="Previous page: Optimization">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="linear_algebra.html" class="navigation navigation-next " aria-label="Next page: Linear Algebra">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"How to prevent overfitting","level":"1.2.2","depth":2,"next":{"title":"Linear Algebra","level":"1.2.3","depth":2,"path":"basics/linear_algebra.md","ref":"basics/linear_algebra.md","articles":[]},"previous":{"title":"Optimization","level":"1.2.1","depth":2,"path":"basics/optimization.md","ref":"basics/optimization.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["katex","-search","-lunr","custom-js-css","docsearch","intopic-toc"],"pluginsConfig":{"docsearch":{},"docSearch":{"apiKey":"9b2d0c53eb838616448e652b3caafb42","index":"ztlevi-machine-learning"},"intopic-toc":{"selector":".markdown-section h1, .markdown-section h2, .markdown-section h3","mode":"nested","maxDepth":6,"isCollapsed":true,"isScrollspyActive":true,"visible":true,"label":"Navigation"},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"custom-js-css":{"css":["css/docsearch.css"]},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"basics/how_to_prevent_overfitting.md","mtime":"2021-06-06T08:01:39.081Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-06-06T08:02:29.611Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-docsearch/doc-search-lib.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-docsearch/doc-search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-intopic-toc/anchor.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-intopic-toc/gumshoe.polyfills.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-intopic-toc/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

