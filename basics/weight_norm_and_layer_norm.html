
<!DOCTYPE HTML>
<html lang>
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Weight norm and layer norm &#xB7; GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content>
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-custom-js-css/9eb07385451571ed1c43d6bfc772c8d5-docsearch.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-docsearch/doc-search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-intopic-toc/style.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Confidence_Interval.html">
    
    
    <link rel="prev" href="calculate_parameters_in_cnn.html">
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary"><div id="book-search-input">
    <input type="text" id="book-doc-search-input" placeholder="Type to search">
</div>
        
            
            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="./">
            
                <a href="./">
            
                    
                    Basics
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="optimization.html">
            
                <a href="optimization.html">
            
                    
                    Optimization
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="how_to_prevent_overfitting.html">
            
                <a href="how_to_prevent_overfitting.html">
            
                    
                    How to prevent overfitting
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="linear_algebra.html">
            
                <a href="linear_algebra.html">
            
                    
                    Linear Algebra
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="clustering.html">
            
                <a href="clustering.html">
            
                    
                    Clustering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="calculate_parameters_in_cnn.html">
            
                <a href="calculate_parameters_in_cnn.html">
            
                    
                    Calculate Parameters in CNN
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.6" data-path="weight_norm_and_layer_norm.html">
            
                <a href="weight_norm_and_layer_norm.html">
            
                    
                    Weight norm and layer norm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="Confidence_Interval.html">
            
                <a href="Confidence_Interval.html">
            
                    
                    Confidence Interval
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="Quantization.html">
            
                <a href="Quantization.html">
            
                    
                    Quantization
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../classical_ml/">
            
                <a href="../classical_ml/">
            
                    
                    Classical Machine Learning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../neural_networks/">
            
                <a href="../neural_networks/">
            
                    
                    Neural Networks
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../neural_networks/different_types_of_convolution.html">
            
                <a href="../neural_networks/different_types_of_convolution.html">
            
                    
                    Different Types of Convolution
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="../neural_networks/loss/">
            
                <a href="../neural_networks/loss/">
            
                    
                    Loss
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.2.1" data-path="../neural_networks/loss/Hinge_Loss.html">
            
                <a href="../neural_networks/loss/Hinge_Loss.html">
            
                    
                    Hinge Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.2" data-path="../neural_networks/loss/cross_entropy_loss.html">
            
                <a href="../neural_networks/loss/cross_entropy_loss.html">
            
                    
                    Cross-Entropy Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.3" data-path="../neural_networks/loss/binary_cross_entropy_loss.html">
            
                <a href="../neural_networks/loss/binary_cross_entropy_loss.html">
            
                    
                    Binary Cross-Entropy Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.4" data-path="../neural_networks/loss/categorical_cross_entropy_loss.html">
            
                <a href="../neural_networks/loss/categorical_cross_entropy_loss.html">
            
                    
                    Categorical Cross-Entropy Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.5" data-path="../neural_networks/loss/focal_loss.html">
            
                <a href="../neural_networks/loss/focal_loss.html">
            
                    
                    Optional: Focal Loss
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.6" data-path="../neural_networks/loss/coral_loss.html">
            
                <a href="../neural_networks/loss/coral_loss.html">
            
                    
                    Optional: CORAL Loss
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="../neural_networks/resnet.html">
            
                <a href="../neural_networks/resnet.html">
            
                    
                    Resnet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="../neural_networks/mobilenet.html">
            
                <a href="../neural_networks/mobilenet.html">
            
                    
                    Mobilenet
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../cv/">
            
                <a href="../cv/">
            
                    
                    Computer Vision
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../cv/two-stage-detector/">
            
                <a href="../cv/two-stage-detector/">
            
                    
                    Two Stage Object Detection
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1.1" data-path="../cv/two-stage-detector/metrics.html">
            
                <a href="../cv/two-stage-detector/metrics.html">
            
                    
                    Metrics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.2" data-path="../cv/two-stage-detector/roi.html">
            
                <a href="../cv/two-stage-detector/roi.html">
            
                    
                    ROI
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.3" data-path="../cv/two-stage-detector/r-cnn.html">
            
                <a href="../cv/two-stage-detector/r-cnn.html">
            
                    
                    R-CNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.4" data-path="../cv/two-stage-detector/fast-rcnn.html">
            
                <a href="../cv/two-stage-detector/fast-rcnn.html">
            
                    
                    Fast RCNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.5" data-path="../cv/two-stage-detector/faster-rcnn.html">
            
                <a href="../cv/two-stage-detector/faster-rcnn.html">
            
                    
                    Faster RCNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.6" data-path="../cv/two-stage-detector/mask-rcnn.html">
            
                <a href="../cv/two-stage-detector/mask-rcnn.html">
            
                    
                    Mask RCNN
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../cv/one-stage-detector/">
            
                <a href="../cv/one-stage-detector/">
            
                    
                    One Stage Object Detection
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.2.1" data-path="../cv/one-stage-detector/yolo.html">
            
                <a href="../cv/one-stage-detector/yolo.html">
            
                    
                    YOLO
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2.2" data-path="../cv/one-stage-detector/ssd.html">
            
                <a href="../cv/one-stage-detector/ssd.html">
            
                    
                    Single Shot MultiBox Detector(SSD)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2.3" data-path="../cv/one-stage-detector/fpn.html">
            
                <a href="../cv/one-stage-detector/fpn.html">
            
                    
                    FPN
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../cv/segmentation/">
            
                <a href="../cv/segmentation/">
            
                    
                    Segmentation
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.3.1" data-path="../cv/segmentation/panoptic.html">
            
                <a href="../cv/segmentation/panoptic.html">
            
                    
                    Panoptic Segmentation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3.2" data-path="../cv/segmentation/PSPNet.html">
            
                <a href="../cv/segmentation/PSPNet.html">
            
                    
                    PSPNet
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="../cv/facenet.html">
            
                <a href="../cv/facenet.html">
            
                    
                    FaceNet
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="../cv/gan.html">
            
                <a href="../cv/gan.html">
            
                    
                    GAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" data-path="../cv/Object_detection_imbalance.html">
            
                <a href="../cv/Object_detection_imbalance.html">
            
                    
                    Imbalance problem in object detection
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../nlp/">
            
                <a href="../nlp/">
            
                    
                    NLP
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../nlp/embedding.html">
            
                <a href="../nlp/embedding.html">
            
                    
                    Embedding
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../nlp/rnn.html">
            
                <a href="../nlp/rnn.html">
            
                    
                    RNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="../nlp/lstm.html">
            
                <a href="../nlp/lstm.html">
            
                    
                    LSTM
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../parallel-computeing/">
            
                <a href="../parallel-computeing/">
            
                    
                    Parallel Computeing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../parallel-computeing/communication.html">
            
                <a href="../parallel-computeing/communication.html">
            
                    
                    Communication
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../parallel-computeing/sync_mapreduce.html">
            
                <a href="../parallel-computeing/sync_mapreduce.html">
            
                    
                    MapReduce
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../parallel-computeing/parameter_server.html">
            
                <a href="../parallel-computeing/parameter_server.html">
            
                    
                    Parameter Server
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../parallel-computeing/decentralized_and_ring_allreduce.html">
            
                <a href="../parallel-computeing/decentralized_and_ring_allreduce.html">
            
                    
                    Decentralized And Ring All Reduce
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../parallel-computeing/federated_learning.html">
            
                <a href="../parallel-computeing/federated_learning.html">
            
                    
                    Federated Learning
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../anomaly-detection/">
            
                <a href="../anomaly-detection/">
            
                    
                    Anomaly Detection
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="../anomaly-detection/dbscan.html">
            
                <a href="../anomaly-detection/dbscan.html">
            
                    
                    DBSCAN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="../anomaly-detection/autoencoder_anomaly.html">
            
                <a href="../anomaly-detection/autoencoder_anomaly.html">
            
                    
                    Autoencoder
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../visualization/">
            
                <a href="../visualization/">
            
                    
                    Visualization
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="../visualization/saliency_maps.html">
            
                <a href="../visualization/saliency_maps.html">
            
                    
                    Saliency Maps
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="../visualization/fooling_images.html">
            
                <a href="../visualization/fooling_images.html">
            
                    
                    Fooling images
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="../visualization/class_visualization.html">
            
                <a href="../visualization/class_visualization.html">
            
                    
                    Class Visualization
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="..">Weight norm and layer norm</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <h2 id="weight-normalization">Weight Normalization</h2>
<p><a href="https://arxiv.org/pdf/1602.07868.pdf" target="_blank">Weight normalization</a> is a method developed by Open AI that, instead of normalizing the mini-batch, <strong>normalizes the weights of the layer</strong>.</p>
<p>Weight normalization reparameterizes the weights <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> of any layer in the neural network in the following way:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mfrac><mrow><mi>g</mi></mrow><mrow><mi mathvariant="normal">&#x2223;</mi><mi mathvariant="normal">&#x2223;</mi><mi>v</mi><mi mathvariant="normal">&#x2223;</mi><mi mathvariant="normal">&#x2223;</mi></mrow></mfrac><mi>v</mi></mrow><annotation encoding="application/x-tex">
w = \frac{g}{||v||} v
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.10756em;"></span><span class="strut bottom" style="height:2.0435600000000003em;vertical-align:-0.936em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="mopen sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.686em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">&#x2223;</span><span class="mord mathrm">&#x2223;</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mord mathrm">&#x2223;</span><span class="mord mathrm">&#x2223;</span></span></span></span><span style="top:-0.2300000000000001em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span><span class="mclose sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span></span></p>
<p>Similar to batch normalization, weight normalization does not reduce the expressive power of the network. What it does is it <strong>separates the norm of the weight vector from its direction</strong>. It then optimizes both <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">g</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span> using gradient descent. This change in learning dynamics makes optimization easier, as I have explained in the <a href="http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/" target="_blank">previous post</a>.</p>
<p><strong>Advantages</strong></p>
<p>Other than the mean and variance being independent of the batch, weight normalization is often <strong>much faster</strong> than batch normalization. In convolutional neural networks, the number of weights tends to be far smaller than the number of inputs, meaning weight normalization is computationally cheaper compared to batch normalization. Batch normalization requires passing through all the elements of the input, which can be extremely expensive, especially when the dimensionality of the input is high, such as in the case of images. Convolutions use the same filter at multiple locations, so a pass through the weights is a lot faster.</p>
<p>Although weight normalization on its own can assist training, the authors of the paper proposed using a method called &quot;mean-only batch normalization&quot; in conjunction with weight normalization. This method is the same as batch normalization except it does not divide the inputs by the standard deviation or rescale them. Though this method counteracts some of the computational speed-up of weight normalization, it is cheaper than batch-normalization since it does not need to compute the standard deviations. The authors claim that this method provides the following benefits:</p>
<ol>
<li>It makes the mean of the activations independent from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">v</span></span></span></span> Weight normalization independently cannot isolate the mean of the activations from the weights of the layer, causing high-level dependencies between the means of each layer. Mean-only batch normalization can resolve this problem.</li>
<li>It adds &quot;gentler noise&quot; to the activations</li>
</ol>
<p>One of the side-effects of batch normalization is that it adds some stochastic noise to the activations as a result of using noisy estimates computed on the mini-batches. This has a regularization effect in some applications but can be potentially harmful in some noise-sensitive domains like reinforcement learning. The noise caused by the mean estimations, however, are &quot;gentler&quot; since the law of large numbers ensures the mean of the activations is approximately normally distributed.</p>
<p>The experimental results of the paper show that weight normalization combined with mean-only batch normalization achieves the best results on CIFAR-10, an image classification dataset. For detailed experimental results, please refer to the original paper.</p>
<h2 id="3-layer-normalization">3. Layer Normalization</h2>
<p><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank">Layer normalization</a> is a method developed by Geoffery Hinton. Compared to weight normalization, layer normalization is slightly harder to grasp intuitively.</p>
<p>To understand layer normalization, recall that a mini-batch consists of multiple examples with the same number of features. Mini-batches are matrices - or tensors if each input is multi-dimensional - where one axis corresponds to the batch and the other axis - or axes - correspond to the feature dimensions.</p>
<p>Batch normalization normalizes the input features across the batch dimension. The key feature of layer normalization is that it <strong>normalizes the inputs across the features</strong>.</p>
<p>The equations of batch normalization and layer normalization are deceptively similar:</p>
<p>Batch normalization:</p>
<p><img src="../.gitbook/assets/layer_norm_1.png" alt></p>
<p>Layer normalization:</p>
<p><img src="../.gitbook/assets/layer_norm_2.png" alt></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span><span class="reset-textstyle scriptstyle cramped mtight"><span class="mord scriptstyle cramped mtight"><span class="mord mathit mtight">i</span><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">&#x200B;</span></span>&#x200B;</span></span></span></span></span></span></span> is the i,j-th element of the input, the first dimension represents the batch and the second represents the feature (I have modified the notation from the original papers to make the contrast clearer).</p>
<p>The difference becomes a bit clearer when we visualize it:</p>
<p><img src="../.gitbook/assets/layer_norm_3.png" alt></p>
<p>In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are <strong>independent of other examples</strong>.</p>
<p>This means that layer normalization is not a simple reparameterization of the network, unlike the case of weight normalization and batch normalization, which both have the same expressive power as an unnormalized neural network. A detailed discussion of the differences this creates involves a lot of math and is beyond the scope of this article, so if you are interested please refer to the original paper.</p>
<p><strong>Advantages</strong></p>
<p>The independence between inputs means that each input has a different normalization operation, allowing arbitrary mini-batch sizes to be used.</p>
<p>The experimental results show that layer normalization performs well for recurrent neural networks.</p>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="calculate_parameters_in_cnn.html" class="navigation navigation-prev " aria-label="Previous page: Calculate Parameters in CNN">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="Confidence_Interval.html" class="navigation navigation-next " aria-label="Next page: Confidence Interval">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Weight norm and layer norm","level":"1.2.6","depth":2,"next":{"title":"Confidence Interval","level":"1.2.7","depth":2,"path":"basics/Confidence_Interval.md","ref":"basics/Confidence_Interval.md","articles":[]},"previous":{"title":"Calculate Parameters in CNN","level":"1.2.5","depth":2,"path":"basics/calculate_parameters_in_cnn.md","ref":"basics/calculate_parameters_in_cnn.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["katex","-search","-lunr","custom-js-css","docsearch","intopic-toc"],"pluginsConfig":{"docsearch":{},"docSearch":{"apiKey":"9b2d0c53eb838616448e652b3caafb42","index":"ztlevi-machine-learning"},"intopic-toc":{"selector":".markdown-section h1, .markdown-section h2, .markdown-section h3","mode":"nested","maxDepth":6,"isCollapsed":true,"isScrollspyActive":true,"visible":true,"label":"Navigation"},"katex":{},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"custom-js-css":{"css":["css/docsearch.css"]},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"basics/weight_norm_and_layer_norm.md","mtime":"2021-06-08T22:44:18.006Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-06-08T22:45:04.538Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-docsearch/doc-search-lib.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-docsearch/doc-search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-intopic-toc/anchor.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-intopic-toc/gumshoe.polyfills.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-intopic-toc/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

